package camml.core.search;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Random;
import java.util.Set;
import java.util.TreeMap;

import org.apache.commons.math3.random.MersenneTwister;
import camml.core.library.StructureFN;
import camml.core.models.ModelLearner;
import camml.core.models.ModelLearner.LearnerException;
import camml.core.models.cpt.CPT;
import camml.core.models.cpt.CPTLearner;
import camml.core.models.multinomial.MLMultinomialLearner;
import camml.plugin.tetrad4.Tetrad4Latent;
import camml.plugin.weka.Converter;
import cdms.core.Type;
import cdms.core.Value;
import cdms.core.VectorFN;
import cdms.core.VectorFN.WeightedVector;

/**
 * This extended version is used to calculate KL distance, MML score, etc.
 * 
 * Time of last edit: Sep 06, 2017 
 * 
 * Author: Xuhui Zhang
 * 
 * The EM algorithm for estimating parameters of the latent variable as well as its children.
 * 
 * In Netica, the cpt of C is constructed as (assume A has states by A1 and A2, B has B1 and B2, 
 * and they have a common child C with C1 and C2):
 *               
 *     A  B  |  C1  C2
 *   ------------------
 *    A1  B1 |  10  90
 *    A1  B2 |  90  10
 *    A2  B1 |  30  70
 *    A2  B2 |  70  30
 *     
 * However, in CaMML, the same cpt will shown as:
 *    
 *     A  B  |  C1  C2
 *   ------------------
 *    A1  B1 |  10  90
 *    A2  B1 |  30  70
 *    A1  B2 |  90  10
 *    A2  B2 |  70  30
 *     
 * 
 * For the same parameters, the netica and CaMML have different parent combination orders. For example, 
 * if the netica cpt is:
 * 
 *   N1     N2  | N3_S1  N3_S2  N3_S3
 * -----------------------------------
 * N1_S1  N2_S1 |  80     10     10
 * N1_S1  N2_S2 |  20     20     60
 * N1_S2  N2_S1 |  50     40     10
 * N1_S2  N2_S2 |   5     90      5
 * 
 * the same cpt will be presented by CaMML as:
 * 
 *   N1     N2  | N3_S1  N3_S2  N3_S3
 * -----------------------------------
 * N1_S1  N2_S1 |  80     10     10
 * N1_S2  N2_S1 |  50     40     10
 * N1_S1  N2_S2 |  20     20     60
 * N1_S2  N2_S2 |   5     90      5
 * 
 * the bnt (matlab) has the same order as CaMML, but the nodes (as well as in the data) in bnt must always 
 * be numbered in topological order, i.e., ancestors before descendants.
 * 
 * Additionally, this EM implementation applies Apache Commons Math Mersenne Twister 
 * random number generator.
 * 
 * @ Author: Xuhui Zhang
 * */

public class EM {

	protected TOM tom;

	// Bias used for parameterization, in case there is some counts in the data are
	// 0, could lead to
	// a learned probability of 0, which is not true in reality
	protected final double biasVal = 0.0;

	// protected Value.Vector data;

	// contains header names of every variable
	private String[] headers;

	// the weighted data passed in
	private Value.Vector initial_data_weighted;

	private WeightedVector EM_data_weighted;

	protected Node[] nodes;

	public int latentArity;

	// private String[] latentStates;
	// store all states of each node
	private String[][] allNodesStates;
	// use to generate allNodesStateCombinations:
	private ArrayList<String> statesCombinations;
	// state combinations of each row in CPT (its own state + parents states) for
	// each node (include the latent node):
	private ArrayList<String[][]> allNodesStateCombinations;
	// state combinations of all node states
	private String[][] globalStateCombinations;
	// all joint probabilities correspond to each global state combination in
	// globalStateCombinations
	private ArrayList<Double> globalStateCombinationsJointProbs;
	// all CPT probabilities of all nodes:
	// public static ArrayList<double[][]> allNodesParams;
	public ArrayList<double[]> allNodesParams;
	// initial random parameters:
	private Value.Vector EM_CPT_Probs;
	// data weights are generated by E-Step
	// private double[] weights;
	// maximum to run EM
	private int max_iter;
	// the maximum marginal likelihood score of each variable of the learned
	// parameters for the model:
	private double MLScore;
	// threshold to stop in terms of log maximum likelihood score
	private double threshold;

	// constructor by a given tom (with latent variable) and data (without latent
	// variable)
	public EM(TOM tom, Value.Vector data, int max_iter, int latentArity, double threshold) {

		this.tom = tom;
		// the first node is the latent
		this.tom.getNode(0).setLatent(true);

		// as the given data is already faked, so no need to create new fake data
		this.initial_data_weighted = data;

		this.nodes = new Node[tom.getNumNodes()];
		for (int i = 0; i < tom.getNumNodes(); i++) {
			this.nodes[i] = tom.getNode(i);
		}

		this.allNodesStates = new String[nodes.length][];
		this.statesCombinations = new ArrayList<String>();
		this.allNodesStateCombinations = new ArrayList<String[][]>();
		this.globalStateCombinationsJointProbs = new ArrayList<Double>();
		this.allNodesParams = new ArrayList<double[]>();
		this.max_iter = max_iter;
		this.MLScore = 0.0;
		this.threshold = threshold;
		// initial the arity of the latent variable by 2:
		this.latentArity = latentArity;

		// put all the observed variable states in
		Type.Structured datatype = (Type.Structured) ((Type.Vector) (initial_data_weighted).t).elt;
		headers = datatype.labels;

		for (int i = 0; i < datatype.cmpnts.length; i++) {
			Type.Symbolic type = (Type.Symbolic) datatype.cmpnts[i];
			String[] nodestates = new String[type.ids.length];
			for (int n = 0; n < type.ids.length; n++) {
				nodestates[n] = headers[i] + "_" + type.ids[n];
			}

			allNodesStates[i] = nodestates;
		}
	}

	/** this constructor means the fake data is given */
	public EM(int[][] latent_matrix, Value.Vector data, int max_iter, int latentArity, double threshold) {
		this(Tetrad4Latent.getLatentTOM(latent_matrix, data), data, max_iter, latentArity, threshold);
	}

	// initialise the fake data as well as parameters
	public void Initialise() throws Exception {

		getGlobalCPTStateComb();
		getAllNodeCPTStateComb();
		// make random parameters if the node is either the latent node itself
		// or the child of the latent variable:
		if (EM_CPT_Probs == null && tom == null)
			EM_CPT_Probs = makeRandomParameters(initial_data_weighted);

		if (tom != null)
			// EM_CPT_Probs = tom.makeParameters(SearchPackage.mmlLatentCPTLearner);
			EM_CPT_Probs = tom.makeParameters(SearchPackage.mlCPTLearner);

		// get CPT values per each node
		getAllNodesCPTProbs();

		// initialize all weights (expected counts for the latent node)

		double[] weights = new double[initial_data_weighted.length()];

		for (int i = 0; i < weights.length; i++) {
			if (initial_data_weighted.weight(i) >= 0)
				weights[i] = initial_data_weighted.weight(i);
			else
				weights[i] = 1.0;
		}

		EM_data_weighted = new WeightedVector(initial_data_weighted, weights);
	}

	private Value.Vector makeRandomParameters(Value.Vector data) {
		int numVars = tom.getNumNodes();

		// Create arrays to hold initial structures.
		String name[] = new String[numVars];
		Value.Vector subParents[] = new Value.Vector[numVars];
		Value.Model[] subModel = new Value.Model[numVars];
		Value subModelParam[] = new Value[numVars];
		Value.Structured subParam[] = new Value.Structured[numVars];
		Value.Structured localStructure[] = new Value.Structured[numVars];

		// initialise name
		Type.Structured dataType = (Type.Structured) ((Type.Vector) data.t).elt;
		for (int i = 0; i < name.length; i++) {
			if (dataType.labels != null) {
				name[i] = dataType.labels[i];
			} else {
				name[i] = "var(" + i + ")";
			}
		}

		// set value of parents.
		for (int i = 0; i < subParents.length; i++) {
			subParents[i] = new VectorFN.FastDiscreteVector(nodes[i].parent.clone());
		}

		// set CPT models and parameters for nodes.
		for (int i = 0; i < subModel.length; i++) {
			// check whether the latent node is one parent of the current node:
			boolean hasLatentParent = false;
			for (int n : nodes[i].parent) {
				if (n == 0) {
					hasLatentParent = true;
					break;
				}
			}

			Value.Structured msy = null;
			msy = learnRandomModel(nodes[i], data, i, hasLatentParent);
			subModel[i] = (Value.Model) msy.cmpnt(0);
			subModelParam[i] = msy.cmpnt(2);
		}

		for (int i = 0; i < subParam.length; i++) {
			subParam[i] = new Value.DefStructured(new Value[] { subModel[i], subModelParam[i] });
		}

		// ( [parants], ( subModel, subParam ) )
		for (int i = 0; i < localStructure.length; i++) {
			localStructure[i] = new Value.DefStructured(
					new Value[] { new Value.Str(name[i]), subParents[i], subParam[i] });
		}

		return new VectorFN.FatVector(localStructure);
	}

	private Value.Structured learnRandomModel(Node node, Value.Vector data, int nodeIndex, boolean hasLatentParent) {
		Value.Vector myData = node.dependentVector(data);
		Value.Vector parentData = node.parentView(data);
		int[][] bounds = CPTLearner.getBounds(parentData);
		int[] lwbArray = bounds[0];
		int[] upbArray = bounds[1];

		Value.Model childModel = null;
		try {
			childModel = CPTLearner.getChildModel(myData, MLMultinomialLearner.mlMultinomialLearner);
		} catch (LearnerException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}

		// the total cost of this node;
		double node_cost = 0.0;

		// try {
		CPT model = new CPT(childModel, lwbArray, upbArray);
		Value sufficientStats = model.getSufficient(myData, parentData);
		CPT cptModel = (CPT) model;
		Value.Vector statVector = (Value.Vector) sufficientStats;

		int nodeStateNum = allNodesStates[nodeIndex].length;
		int numParentStateCombinations = allNodesStateCombinations.get(nodeIndex).length / nodeStateNum;

		Value[] paramArray = new Value[numParentStateCombinations];

		int n = 0;
		for (int i = 0; i < numParentStateCombinations; i++) {
			Value.Structured stats = (Value.Structured) statVector.elt(i);

			double params[] = generateRandomProbs(stats.length());
			n++;

			Value.Structured leafMSY = new Value.DefStructured(
					new Value[] { model, stats, new StructureFN.FastContinuousStructure(params) });

			paramArray[i] = new Value.DefStructured(new Value[] { leafMSY.cmpnt(0), leafMSY.cmpnt(2) });
		}

		Value.Vector paramVector = new VectorFN.FatVector(paramArray);

		return new Value.DefStructured(new Value[] { cptModel, statVector, paramVector });
	}

	// re-calculate parameters using expected counts:
	private Value.Structured learnUpdateModel(Node node, Value.Vector data, int nodeIndex) {

		// update the total cost of this node;
		double node_cost = 0.0;

		String[][] nodeStateCombinatons = allNodesStateCombinations.get(nodeIndex);
		Value.Vector myData = node.dependentVector(data);
		Value.Vector parentData = node.parentView(data);
		int[][] bounds = CPTLearner.getBounds(parentData);
		int[] lwbArray = bounds[0];
		int[] upbArray = bounds[1];

		Value.Model childModel = null;
		try {
			childModel = CPTLearner.getChildModel(myData, MLMultinomialLearner.mlMultinomialLearner);
		} catch (LearnerException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}

		// try {
		CPT model = new CPT(childModel, lwbArray, upbArray);
		Value sufficientStats = model.getSufficient(myData, parentData);
		CPT cptModel = (CPT) model;
		Value.Vector statVector = (Value.Vector) sufficientStats;
		int nodeStateNum = allNodesStates[nodeIndex].length;
		// number of rows in the cpt
		int numCPTRows = allNodesStateCombinations.get(nodeIndex).length / nodeStateNum;

		// to update allNodesParams:
		double[] newValues = new double[allNodesStateCombinations.get(nodeIndex).length];

		Value[] paramArray = new Value[numCPTRows];

		// loop each row of the expected cpt:
		for (int i = 0; i < numCPTRows; i++) {
			Value.Structured stats = (Value.Structured) statVector.elt(i);
			// Initialise the expected total weights and counts
			double[] weights_total = new double[stats.length()];
			// record sum of weights of all entries of this cpt row
			double all_states_weights_total = 0.0;

			for (int n = 0; n < weights_total.length; n++) {
				// add the bias in every entry
				weights_total[n] = 0.0 + biasVal;
			}
			/**
			 * Get the state combination of each row shown in the cpt: If the cpt is: 
			 *          N3 
			 *  N1 N2 | T F 
			 * ------------------- 
			 *   T T  | 60 40 
			 *   T F  | 50 50 
			 *   F T  | 20 80 
			 *   F F  | 10 90
			 * 
			 * Then the first row state combination will be: {N1=T,N2=T,N3=T} and
			 * {N1=T,N2=T,N3=F}
			 */

			String[][] cptRowStateCombinations = new String[nodeStateNum][];
			for (int n = 0; n < nodeStateNum; n++) {
				String[] stateCombinatons = nodeStateCombinatons[nodeStateNum * i + n];
				cptRowStateCombinations[n] = stateCombinatons;
			}

			for (int n = 0; n < EM_data_weighted.length(); n++) {
				String[] rowdata = new String[nodes.length];
				for (int m = 0; m < nodes.length; m++) {
					int tempIndex = m + 1;
					// rowdata[m] = "Node" + tempIndex + "_" +
					// weightedEMfakedata.cmpnt(m).elt(n).toString();
					rowdata[m] = headers[m] + "_" + EM_data_weighted.cmpnt(m).elt(n).toString();
				}

				int index = getStateCombMatchedIndex(rowdata, cptRowStateCombinations);
				// if matched:
				if (index != -1) {
					weights_total[index] = weights_total[index] + EM_data_weighted.weight(n);
				}
			}

			for (int n = 0; n < stats.length(); n++) {
				all_states_weights_total += weights_total[n];
			}

			// re-estimate the probabilities:
			double[] params_new = new double[stats.length()];

			for (int n = 0; n < stats.length(); n++) {

				params_new[n] = weights_total[n] / all_states_weights_total;
				newValues[stats.length() * i + n] = params_new[n];
			}

			Value.Structured leafMSY = new Value.DefStructured(
					new Value[] { model, stats, new StructureFN.FastContinuousStructure(params_new) });

			paramArray[i] = new Value.DefStructured(new Value[] { leafMSY.cmpnt(0), leafMSY.cmpnt(2) });

			// update allNodesParams:
			allNodesParams.set(nodeIndex, newValues);
		}

		Value.Vector paramVector = new VectorFN.FatVector(paramArray);

		return new Value.DefStructured(new Value[] { cptModel, statVector, paramVector });

	}

	private double[] generateRandomProbs(int stateNum) {

		// apply the Mersenne Twister random number generator:
		MersenneTwister rand = new MersenneTwister();
		// Random rand = new Random();
		double[] parameters = new double[stateNum];
		double sum = 0.0;

		for (int i = 0; i < parameters.length; i++) {
			parameters[i] = rand.nextDouble();
			sum += parameters[i];
		}

		for (int i = 0; i < parameters.length; i++) {
			parameters[i] /= sum;
		}

		return parameters;
	}

	/**
	 * these random weights depend on how many states of the latent variable.
	 */
	private static double[] generateRandomWeights(int stateNum) {
		// apply the Mersenne Twister random number generator:
		MersenneTwister rand = new MersenneTwister();
		// Random rand = new Random();

		double[] parameters = new double[stateNum];
		double sum = 0.0;

		for (int i = 0; i < parameters.length; i++) {
			parameters[i] = rand.nextDouble();
			sum += parameters[i];
		}

		for (int i = 0; i < parameters.length; i++) {
			parameters[i] /= sum;
		}

		return parameters;
	}

	// get all state combinations of all nodes:
	private void getGlobalCPTStateComb() {
		String[][] reverse_states = adaptStateCombiOrder(allNodesStates);
		recursive_vector(0, "", reverse_states);

		globalStateCombinations = new String[statesCombinations.size()][];
		for (int i = 0; i < statesCombinations.size(); i++) {
			String str = statesCombinations.get(i);

			// adapt to camml order
			String[] temp = str.split(",");

			String[] temp_reverse = new String[temp.length];

			for (int n = 0; n < temp.length; n++) {
				temp_reverse[n] = temp[temp.length - 1 - n];
			}

			globalStateCombinations[i] = temp_reverse;
		}

		statesCombinations = new ArrayList<String>();
	}

	// get CPT state combination for each node:
	private void getAllNodeCPTStateComb() {
		// reset all nodes state combinations (the older version forget to do this, so fixed it here)
		allNodesStateCombinations = new ArrayList<String[][]>();

		for (int i = 0; i < nodes.length; i++) {
			nodes[i] = tom.getNode(i);
		}

		for (int i = 0; i < nodes.length; i++) {
			getCurrentNodeCPTStateComb(nodes[i], i);
		}
	}

	private void getCurrentNodeCPTStateComb(camml.core.search.Node node, int nodeIndex) {
		Type.Structured datatype = (Type.Structured) ((Type.Vector) (initial_data_weighted).t).elt;

		int[] parents = node.getParentCopy();

		String[][] states = new String[parents.length + 1][];

		String[] currentNodeStates = allNodesStates[nodeIndex];

		states[0] = currentNodeStates;

		for (int i = 0; i < parents.length; i++) {
			Type.Symbolic parentType = (Type.Symbolic) datatype.cmpnts[parents[i]];

			String[] parentStates = new String[parentType.ids.length];

			for (int n = 0; n < parentType.ids.length; n++) {
				int tempIndex = parents[i] + 1;
				parentStates[n] = headers[parents[i]] + "_" + parentType.ids[n];
			}

			states[i + 1] = parentStates;
		}

		String[][] reverse_states = adaptStateCombiOrder(states);

		recursive_vector(0, "", reverse_states);

		String[] currentNodeStateCombinations[] = new String[statesCombinations.size()][];
		for (int i = 0; i < statesCombinations.size(); i++) {
			String str = statesCombinations.get(i);
			// adapt to camml order:
			String[] temp = str.split(",");

			String[] temp_reverse = new String[temp.length];

			for (int n = 0; n < temp.length; n++) {
				temp_reverse[n] = temp[temp.length - 1 - n];
			}

			currentNodeStateCombinations[i] = temp_reverse;
		}

		allNodesStateCombinations.add(currentNodeStateCombinations);

		statesCombinations = new ArrayList<String>();
	}

	private String[][] adaptStateCombiOrder(String[][] states) {
		String[][] reverse_states = new String[states.length][];

		for (int i = 0; i < states.length; i++) {
			reverse_states[states.length - 1 - i] = states[i];
		}

		return reverse_states;
	}

	// recursively generate all possible states combinations
	private void recursive_vector(int d, String str, String[][] states) {
		if (d == states.length) {
			statesCombinations.add(str);
			return;
		}
		for (int k = 0; k < states[d].length; k++) {
			recursive_vector(d + 1, str + states[d][k] + ",", states);
		}
		return;
	}

	// get all nodes CPT probs based on current parameters
	private void getAllNodesCPTProbs() {
		// for each node:
		for (int i = 0; i < nodes.length; i++) {
			Value.Vector vec = getNodeProb(i);

			ArrayList<Double> temp = new ArrayList<Double>();
			// access to each row:
			for (int n = 0; n < vec.length(); n++) {
				Value.Structured elt = (Value.Structured) vec.elt(n);
				Value.Structured subParams = (Value.Structured) elt.cmpnt(1);
				for (int j = 0; j < subParams.length(); j++) {
					temp.add(subParams.doubleCmpnt(j));
				}
			}

			double[] nodeCPTProbs = new double[temp.size()];
			for (int m = 0; m < temp.size(); m++) {
				nodeCPTProbs[m] = temp.get(m);
			}

			allNodesParams.add(nodeCPTProbs);
		}
	}

	private int getLatentNodeIndex() {
		int index = 0;
		for (int i = 0; i < nodes.length; i++) {
			if (nodes[i].latent == true) {
				index = i;
				break;
			}
		}
		return index;
	}

	// get the index of the direct children of the latent variable
	private int[] getLatentChildren(int latentIndex) {
		ArrayList<Integer> result = new ArrayList<Integer>();
		for (int i = 0; i < nodes.length; i++) {
			int[] parents = nodes[i].parent;
			for (int n = 0; n < parents.length; n++) {
				if (parents[n] == latentIndex)
					result.add(i);
			}
		}

		int[] childrenIndex = new int[result.size()];
		for (int i = 0; i < result.size(); i++) {
			childrenIndex[i] = result.get(i);
		}
		return childrenIndex;
	}

	// get a selected probability based on a chosen variable as well as its state
	private Value.Vector getNodeProb(int varIndex) {
		Value.Vector vec = (Value.Vector) EM_CPT_Probs;

		Value[] value = new Value[vec.length()];

		for (int i = 0; i < value.length; i++) {
			Value.Structured temp = (Value.Structured) vec.elt(i);
			value[i] = ((Value.Structured) (temp.cmpnt(2))).cmpnt(1);
		}

		Value.Vector var_vec = (Value.Vector) value[varIndex];

		return var_vec;
	}

	// get the index of the matched node state combination for the current global
	// state combination:
	private int getStateCombMatchedIndex(String[] globalCombination, String[][] nodeStateCombinations) {
		int index = -1;

		for (int i = 0; i < nodeStateCombinations.length; i++) {
			boolean match = true;
			// check which state combination of the node matches the given global state
			// combination:
			String[] nodeStateComb = nodeStateCombinations[i];

			for (String nodeState : nodeStateComb) {
				boolean hasStates = false;
				for (String globalState : globalCombination) {
					if (nodeState.equals(globalState)) {
						hasStates = true;
						continue;
					}
				}
				if (hasStates == false) {
					match = false;
					break;
				}
			}
			if (match == false)
				continue;
			if (match == true) {
				index = i;
				break;
			}
		}

		return index;
	}

	public void Run() throws Exception {

		Initialise();

		double oldMLScore = 0.0;

		for (int i = 0; i < max_iter; i++) {

			EStep();
			MStep();
			/**
			 * Same as bnt: threshold specifies the threshold for stopping EM. Default:
			 * 1e-3. We stop when |f(t) - f(t-1)| / avg < threshold, where avg = (|f(t)| +
			 * |f(t-1)|)/2 and f is log likelihood.
			 * 
			 */
			double newMLScore = getMLScore();

			double oldPlusNewScore = Math.abs(oldMLScore) + Math.abs(newMLScore);

			double avgMLScore = (oldPlusNewScore + Math.ulp(oldPlusNewScore)) / 2;

			double deltaScore = Math.abs(newMLScore - oldMLScore);

			if ((deltaScore / avgMLScore) < threshold)
				break;

			// System.out.println("EM Iteration " + i + " Score:" + getMLScore());
			// System.out.println("Iteration " + i + " CaMML Score: " + getCaMML_MLCost(tom,
			// tom.getNumNodes(), CPTLearner.mlMultinomialCPTLearner,
			// getWeightedEMFakeData()));

			// System.out.print(".");
			oldMLScore = MLScore;

			MLScore = 0.0;

			System.out.print(".");
			if (i % 100 == 0 && i > 0)
				System.out.println();

		}
	}

	public void EStep() {

		// reset all joint probabilities:
		globalStateCombinationsJointProbs = new ArrayList<Double>();

		// all weights corresponding to each different state combination:
		double[] stateComb_weights = new double[globalStateCombinations.length];
		// we need to record subTotal cuz the joint probability of each row is
		// new_weight* subTotal;
		double[] subTotals = new double[globalStateCombinations.length];

		/**
		 * all the sub totals (sum of joint probabilities) that will be used to
		 * calculate the marginal log likelihood score:
		 * 
		 * for each row of data, new_weight = sub Joint Probability / sub total
		 * probability.
		 * 
		 * for example, if we have four observed variables (A, B, C, D), and if one row
		 * of the original data is T, T, F, T and the latent variable (H) has two states
		 * (H1, H2),
		 * 
		 * then the joint probability of this row is the sum of the two joint
		 * probabilities Joint Probability
		 * 
		 * P(A=T, B=T, C=F, D=F) = P(H=H1, A=T, B=T, C=F, D=F) + P(H=H2, A=T, B=T, C=F,
		 * D=F)
		 * 
		 * So the new weights for the data row (A=T, B=T, C=F, D=F) are:
		 * 
		 * P(H=H1, A=T, B=T, C=F, D=F)/P(A=T, B=T, C=F, D=F) and P(H=H2, A=T, B=T, C=F,
		 * D=F)/P(A=T, B=T, C=F, D=F)
		 */

		int loop = 0;

		while (loop <= globalStateCombinations.length - latentArity) {
			double[] subJointProbs = new double[latentArity];
			double subTotal = 0.0;

			// because the number that the original data is copied depends on how many
			// states the latent variable has.
			for (int m = 0; m < latentArity; m++) {
				ArrayList<Double> matchedProbs = new ArrayList<Double>();
				String[] globalStateCombination = globalStateCombinations[loop + m];
				double jointProb = 1.0;

				for (int j = 0; j < nodes.length; j++) {
					String[][] nodeStateCombinations = allNodesStateCombinations.get(j);
					int stateCombMatchedIndex = getStateCombMatchedIndex(globalStateCombination, nodeStateCombinations);
					// store the corresponding matched probability
					matchedProbs.add(allNodesParams.get(j)[stateCombMatchedIndex]);
				}

				for (double prob : matchedProbs) {
					jointProb *= prob;
				}
				globalStateCombinationsJointProbs.add(jointProb);
				subJointProbs[m] = jointProb;
				subTotal += subJointProbs[m];
			}
			// calculate the subWeights:
			for (int i = 0; i < latentArity; i++) {
				stateComb_weights[loop + i] = subJointProbs[i] / subTotal;
				subTotals[loop + i] = subTotal;
			}

			loop += latentArity;
		}

		// revise the weights of the weighted data:
		double[] new_weights = new double[EM_data_weighted.length()];

		for (int i = 0; i < EM_data_weighted.length(); i++) {
			// rename each cell of the data:
			String[] rowdata = new String[nodes.length];
			for (int n = 0; n < nodes.length; n++) {
				int tempIndex = n + 1;

				rowdata[n] = headers[n] + "_" + EM_data_weighted.cmpnt(n).elt(i).toString();
			}

			for (int m = 0; m < globalStateCombinations.length; m++) {
				String[] globalStateCombination = globalStateCombinations[m];
				if (Arrays.equals(rowdata, globalStateCombination)) {
					new_weights[i] = stateComb_weights[m];
					
					// in case the number is too small or NaN, set the value to the smallest double. This needs to be fixed in future.
					if(Double.isNaN(new_weights[i]))
						new_weights[i] = Double.MIN_VALUE;
					
					break;
				}
			}

		}

		// revise the weights and return the new weighted data;
		EM_data_weighted = new WeightedVector(initial_data_weighted, new_weights);

		ComputeMLScore(subTotals);
	}

	public void MStep() {
		int numVars = tom.getNumNodes();

		// Create arrays to hold initial structures.
		String name[] = new String[numVars];
		Value.Vector subParents[] = new Value.Vector[numVars];
		Value.Model[] subModel = new Value.Model[numVars];
		Value subModelParam[] = new Value[numVars];
		Value.Structured subParam[] = new Value.Structured[numVars];
		Value.Structured localStructure[] = new Value.Structured[numVars];

		// initialise name
		Type.Structured dataType = (Type.Structured) ((Type.Vector) initial_data_weighted.t).elt;
		for (int i = 0; i < name.length; i++) {
			if (dataType.labels != null) {
				name[i] = dataType.labels[i];
			} else {
				name[i] = "var(" + i + ")";
			}
		}

		// set value of parents.
		for (int i = 0; i < subParents.length; i++) {
			subParents[i] = new VectorFN.FastDiscreteVector(nodes[i].parent.clone());
		}

		// set CPT models and parameters for nodes.
		for (int i = 0; i < subModel.length; i++) {
			// check whether the latent node is one parent of the current node:
			boolean hasLatentParent = false;
			for (int n : nodes[i].parent) {
				if (n == 0) {
					hasLatentParent = true;
					break;
				}
			}
			/**
			 * if the current node is the latent variable (with index of 0), or if the
			 * latent is not the parent of the current node, learn the parameters by normal
			 * counting; otherwise re-estimate the parameters by the weighted data:
			 * 
			 * TODO: maybe copy from the existing params will make it a bit quicker, cuz we
			 * dont need to learn the parameters of a node which is not associated with the
			 * latent.
			 */
			Value.Structured msy = null;
			msy = learnUpdateModel(nodes[i], initial_data_weighted, i);

			subModel[i] = (Value.Model) msy.cmpnt(0);
			subModelParam[i] = msy.cmpnt(2);
		}

		for (int i = 0; i < subParam.length; i++) {
			subParam[i] = new Value.DefStructured(new Value[] { subModel[i], subModelParam[i] });
		}

		// ( [parants], ( subModel, subParam ) )
		for (int i = 0; i < localStructure.length; i++) {
			localStructure[i] = new Value.DefStructured(
					new Value[] { new Value.Str(name[i]), subParents[i], subParam[i] });
		}

		EM_CPT_Probs = new VectorFN.FatVector(localStructure);

	}

	public WeightedVector getWeightedEMFakeData() {
		return EM_data_weighted;
	}

	public Value.Vector getBestParameters() throws LearnerException {
		return EM_CPT_Probs;
	}

	/**
	 * This function is to get the log-likelihood function cost (should run after
	 * updating weights).
	 */
	private void ComputeMLScore(double[] subTotals) {
		// reset the marginal likelihood score:
		MLScore = 0.0;

		for (int i = 0; i < EM_data_weighted.length(); i++) {
			// rename each cell of the data:
			String[] rowdata = new String[nodes.length];
			for (int n = 0; n < nodes.length; n++) {
				int tempIndex = n + 1;
				// rowdata[n] = "Node" + tempIndex + "_" +
				// weightedEMfakedata.cmpnt(n).elt(i).toString();
				rowdata[n] = headers[n] + "_" + EM_data_weighted.cmpnt(n).elt(i).toString();
			}

			for (int m = 0; m < globalStateCombinations.length; m++) {

				String[] globalStateCombination = globalStateCombinations[m];

				if (Arrays.equals(rowdata, globalStateCombination)) {

					try {
						MLScore -= java.lang.Math.log(globalStateCombinationsJointProbs.get(m))
								* EM_data_weighted.weight(i);
					} catch (Exception e) {
						System.out.println("globalStateCombinationsJointProbs.get(m): "
								+ globalStateCombinationsJointProbs.get(m));
						System.out.println("java.lang.Math.log(globalStateCombinationsJointProbs.get(m)): "
								+ java.lang.Math.log(globalStateCombinationsJointProbs.get(m)));
					}

					break;
				}
			}
		}

	}

	/** Update structure if necessary (for running MCMC inside EM). */
	public void updateStructure(TOM tom1) {
		this.tom = tom1;
		for (int i = 0; i < tom.getNumNodes(); i++) {
			this.nodes[i] = tom.getNode(i);
		}
		// update the nodes CPT state combination as well.
		getAllNodeCPTStateComb();
	}

	// get the marginal log likelihood score
	public double getMLScore() {
		return MLScore;
	}

}
